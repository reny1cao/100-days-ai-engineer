---
title: Week 4 — ML & LLM Fundamentals (Concepts)
description: ML basics, Transformers/attention, prompting vs fine-tuning, RLHF, and LLM evaluation.
---

Focus

Refresh ML basics and LLM concepts: Transformer intuition, prompting/fine-tuning, RLHF, and evaluation. This supports RAG and agent design.

Learning Goals

- Explain how LLMs work at a high level (next-token prediction, attention).
- Understand training vs inference, overfitting, common metrics; prompt engineering techniques; why RLHF is used.

Key Connections

Theory underpins Week 5 (RAG) and Week 6 (Agents). Model behavior informs system design and serving decisions.

Study Resources

- ML Fundamentals: Supervised vs unsupervised, validation, bias-variance (Ng’s notes or a concise summary).
- LLM & Transformer: “Illustrated Transformer” or beginner videos.
- Prompting & RLHF: OpenAI/LangChain guides; RLHF intros.
- LLM Evaluation: Perplexity limits, automated vs human eval, safety/guardrails.

Practice & Activities

- Flashcards: Key terms (transfer learning, embeddings, RAG vs fine-tuning).
- ELI5: Write simple explanations for attention, embeddings, overfitting.
- Mini-project: Prompt a model (API or local) with/without context; note behavior.

Checkpoint & Reflection

Answer: “What is RAG and why is it important?” and “What are components of an AI agent?” with concise, structured responses. Connect to your past work (evals, guardrails).

